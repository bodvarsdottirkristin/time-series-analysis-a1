---
title: "R Notebook"
output: html_notebook
---
```{r}
library(ggplot2)
```

```{r}
### Read training data
#! Perhaps you need to set the working directory!?
#setwd("/home/pbac/g/course02417/2025/assignment1")
D <- read.csv("DST_BIL54.csv")
str(D)

# See the help
?strftime
D$time <- as.POSIXct(paste0(D$time,"-01"), "%Y-%m-%d", tz="UTC")
D$time
class(D$time)

## Year to month for each of them
D$year <- 1900 + as.POSIXlt(D$time)$year + as.POSIXlt(D$time)$mon / 12

## Make the output variable a floating point (i.e.\ decimal number)
D$total <- as.numeric(D$total) / 1E6

## Divide intro train and test set
teststart <- as.POSIXct("2024-01-01", tz="UTC")
Dtrain <- D[D$time < teststart, ]
Dtest <- D[D$time >= teststart, ]
```

```{r}
Dtrain
```
## 1. Plot data

1.1
```{r}
dates <- as.Date(Dtrain$time)

# Create x 
Dtrain$x <- as.numeric(format(Dtrain$time, "%Y")) + 
            (as.numeric(format(Dtrain$time, "%m")) - 1) / 12
```

1.2
```{r}
# Plot training data vs x

variable_of_interest = Dtrain$total
plot(Dtrain$x, variable_of_interest, type = "l", xlab = "Time Variable X", 
     ylab = "Total", main = "Total vs Time Variable X")

```

```{r}
# Basic statistics
cat("Start value (2018-01):", Dtrain$total[1], "\n") 
cat("End value (2023-12):", tail(Dtrain$total, 1), "\n")
cat("Total growth:", tail(Dtrain$total, 1) - Dtrain$total[1], "\n")
cat("Monthly observations:", nrow(Dtrain), "\n")
cat("Average monthly growth:", mean(diff(Dtrain$total)), "\n")
cat("Annual growth rate:", mean(diff(Dtrain$total)) * 12, "\n")

# Linear trend fit (R² for predictability)
fit <- lm(total ~ x, data = Dtrain)
cat("Linear model R²:", round(summary(fit)$r.squared, 3), "\n")

# Print results
print(fit)
```

The training data plot shows a clear upward trend in total registered motor vehicles in Denmark from 2018 to approximately 2022 with slight downward hills in beginning of 2022 and also in the end of 2022 leading up to the start of 2023. 


## 2. Linear trend model

```{r}
head(Dtrain[, c("x", "total")], 3)
```

## 3. OLS - global linear trend model

# 3.1. Estimate the parameters θ1 and θ2 using the training set (call it the Ordinary Least Squares (OLS) estimates). Describe how you calculated the estimates.

```{r}
y <- Dtrain$total
X <- cbind(1, Dtrain$x)

thetahat <- solve(t(X) %*% X) %*% t(X) %*% y

thetahat

```

# 3.2

```{r}

# Step 1: Residuals
residuals <- y - X %*% thetahat

# Step 2: Estimate sigma^2 (residual variance)
n <- nrow(X)           # 72 observations
p <- ncol(X)           # 2 parameters
sigma2_hat <- sum(residuals^2) / (n - p)  # unbiased estimator

# Step 3: Variance-covariance matrix of thetahat
V_theta <- sigma2_hat * solve(t(X) %*% X)

# Step 4: Standard errors = sqrt(diag(V_theta))
se_theta1 <- sqrt(V_theta[1,1])  # σ̂_θ₁
se_theta2 <- sqrt(V_theta[2,2])  # σ̂_θ₂

se_theta1

se_theta2
```

```{r}
fitted_mean <- X %*% thetahat  # μ̂(x) for all training points

plot(Dtrain$x, Dtrain$total, 
     pch = 16, col = "steelblue", cex = 0.8,
     xlab = "Time Variable x (fractional years)",
     ylab = "Total Registered Vehicles",
     main = "Observations and Estimated Mean")

lines(Dtrain$x, fitted_mean, col = "red", lwd = 2)

# Legend
legend("topleft", 
       legend = c("Observations y", "Estimated Mean)"), 
       pch = c(16, NA), 
       lty = c(NA, 1), 
       col = c("steelblue", "red"), 
       lwd = c(NA, 2))

```

# 3.3 Make a forecast for the test set, hence the following 12 months - i.e., compute predicted values
with corresponding prediction intervals for 2024-Jan to 2024-Dec. Present these values in a table.

```{r}
# Test set: 2024-Jan to Dec

# Create x 
Dtest$x <- as.numeric(format(Dtest$time, "%Y")) + 
            (as.numeric(format(Dtest$time, "%m")) - 1) / 12

X_test <- cbind(1, Dtest$x)

# Predictions + 95% prediction intervals
yhat_test <- X_test %*% thetahat

se_pred_test <- sqrt(sigma2_hat + diag(X_test %*% V_theta %*% t(X_test)))
t_crit <- qt(0.975, df = nrow(X) - ncol(X))  # 95% critical t-value

pred_int <- cbind(
  lower = yhat_test - t_crit * se_pred_test,
  upper = yhat_test + t_crit * se_pred_test
)

# Forecast table using Dtest months
forecast_table <- data.frame(
  Month = format(Dtest$time, "%b %Y"),
  x = round(Dtest$x, 3),
  Actual = round(Dtest$total, 3),
  Forecast = round(yhat_test, 3),
  `Lower 95%` = round(pred_int[,1], 3),
  `Upper 95%` = round(pred_int[,2], 3)
)
print(forecast_table, row.names = FALSE)
```


# 3.4 Plot the fitted model together with the training data and the forecasted values (also plot the prediction intervals of the forecasted values).



```{r}

# Order test data
ord <- order(Dtest$x)

# Dataframe for plot
Dpred <- data.frame(
  x = Dtest$x[ord],
  yhat = yhat_test[ord],
  lower = pred_int[ord, 1],
  upper = pred_int[ord, 2]
)


x_seq <- seq(min(c(Dtrain$x, Dtest$x)), 2025, length.out = 200)
Dfitted <- data.frame(
  x = x_seq,
  y = coef(fit)[1] + coef(fit)[2] * x_seq
)

ggplot() +
  geom_point(data = Dtrain,
             aes(x = x, y = total, color = "Training data")) +
  geom_point(data = Dtest,
             aes(x = x, y = total, color = "Test data")) +
  geom_line(data = Dfitted,
            aes(x = x, y = y, color = "Fitted model"),
            linewidth = 1) +
  geom_line(data = Dpred,
            aes(x = x, y = yhat, color = "Forecast"),
            linewidth = 1) +
  geom_line(data = Dpred,
            aes(x = x, y = lower, color = "95% Prediction Interval"),
            linetype = "dashed") +
  geom_line(data = Dpred,
            aes(x = x, y = upper, color = "95% Prediction Interval"),
            linetype = "dashed") +
  
  coord_cartesian(
    xlim = c(min(c(Dtrain$x, Dtest$x)), 2025),
    ylim = c(min(c(Dtrain$total, Dtest$total)), 3.4)) +
  
  scale_color_manual(
    name = "",
    values = c("Training data" = "black",
               "Test data" = "darkgreen",
               "Fitted model" = "blue",
               "Forecast" = "red",
               "95% Prediction Interval" = "red")) +
  
  labs(title = "Fitted Model with Forecast and Prediction Intervals",
       x = "Year",
       y = "Total registered vehicles (millions)") +
  
  theme_minimal() +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.02, 0.98),
    legend.justification = c(0, 0.9))
```

```{r}

X_train <- cbind(1, Dtrain$x)

# Predictions + 95% prediction intervals for training data
yhat_train <- X_train %*% thetahat

se_pred_train <- sqrt(sigma2_hat + diag(X_train %*% V_theta %*% t(X_train)))
t_crit_train <- qt(0.975, df = nrow(X) - ncol(X))  # 95% critical t-value
pred_int_train <- cbind(
  lower = yhat_train - t_crit_train * se_pred_train,
  upper = yhat_train + t_crit_train * se_pred_train
)

```

```{r}
# Order test data
ord_train <- order(Dtrain$x)

# Dataframe for plot
Dpred <- data.frame(
  x = Dtrain$x[ord_train],
  yhat = yhat_train[ord_train],
  lower = pred_int_train[ord_train, 1],
  upper = pred_int_train[ord_train, 2]
)


x_seq <- seq(min(c(Dtrain$x, Dtest$x)), 2024, length.out = 200)
Dfitted <- data.frame(
  x = x_seq,
  y = coef(fit)[1] + coef(fit)[2] * x_seq
)

ggplot() +
  geom_point(data = Dtrain,
             aes(x = x, y = total, color = "Training data")) +
  #geom_point(data = Dtest,
   #          aes(x = x, y = total, color = "Test data")) +
  geom_line(data = Dfitted,
            aes(x = x, y = y, color = "Fitted model"),
            linewidth = 1) +
  #geom_line(data = Dpred,
    #        aes(x = x, y = yhat, color = "Forecast"),
    #        linewidth = 1) +
  geom_line(data = Dpred,
            aes(x = x, y = lower, color = "95% Prediction Interval"),
            linetype = "dashed") +
  geom_line(data = Dpred,
            aes(x = x, y = upper, color = "95% Prediction Interval"),
            linetype = "dashed") +
  
  #coord_cartesian(
   # xlim = c(min(c(Dtrain$x, Dtest$x)), 2025),
    #ylim = c(min(c(Dtrain$total, Dtest$total)), 3.4)) +
  
  scale_color_manual(
    name = "",
    values = c("Training data" = "black",
               "Fitted model" = "red",
               "95% Prediction Interval" = "red")) +
  
  labs(title = "Fitted Model with Forecast and Prediction Intervals",
       x = "Year",
       y = "Total registered vehicles (millions)") +
  
  theme_minimal() +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.02, 0.98),
    legend.justification = c(0, 0.9))
```

# 3.6 Investigate the residuals of the model

```{r}
residuals <- fitted_mean - Dtrain$total

plot(Dtrain$x, residuals,
     pch = 16,
     xlab = "Year",
     ylab = "Residuals",
     main = "Residuals Over Time")
abline(h = 0, col = "red", lwd = 2)
```


```{r}
qqnorm(residuals)
qqline(residuals, col = "red")
```


```{r}
hist(residuals,
     breaks = 15,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```

# 3.1 
```{r}
lambda <- 0.9
lambdas <- lambda^((nrow(Dtrain)-1):0)

plot_data <- data.frame(
  Time = Dtrain$year,
  Weight = lambdas
)


p <- ggplot(plot_data, aes(x = Time, y = Weight)) +
  geom_point() +                      
  labs(x = "Time", y = "Weight") +    
  theme_minimal()                     


print(p)

# 4. Save the plot
#ggsave("4_2.png", plot = p, width = 8, height = 6, dpi = 600)
```
# 4.3
```{r}
lambda_sum = sum(lambdas)
lambda_sum

OLS_sum = nrow(Dtrain)
OLS_sum
```
# 4.4
```{r}
cov = diag(1/lambdas)

X_train <- cbind(1, Dtrain$x)
Y_train <- Dtrain$total 

F_N = t(X_train) %*% solve(cov) %*% X_train
h_N = t(X_train) %*% solve(cov) %*% Y_train

theta_hat_wls <- solve(F_N) %*% h_N
theta_hat_wls
```
# 4.5
```{r}
library(ggplot2)

# ==========================================
# 1. OLS Full Model (Train + Test)
# ==========================================
# existing OLS thetahat and V_theta from sec 3
X_test  <- cbind(1, Dtest$x)
X_full  <- rbind(X_train, X_test)

# OLS Predictions for all data
y_hat_ols_full <- X_full %*% thetahat

# OLS standard errors and intervals for all data
se_pred_ols_full <- sqrt(sigma2_hat + diag(X_full %*% V_theta %*% t(X_full)))
t_crit_ols <- qt(0.975, df = nrow(X_train) - ncol(X_train))

pred_int_ols_full <- cbind(
  lower = y_hat_ols_full - t_crit_ols * se_pred_ols_full,
  upper = y_hat_ols_full + t_crit_ols * se_pred_ols_full
)

df_ols_full <- data.frame(
  x = c(Dtrain$x, Dtest$x),
  yhat = y_hat_ols_full,
  lower = pred_int_ols_full[, 1],
  upper = pred_int_ols_full[, 2],
  Model = "OLS (Global Trend)"
)

# ==========================================
# 2. WLS Full Model (Train + Test)
# ==========================================

# WLS variance and standard errors
res_wls <- Dtrain$total - (X_train %*% theta_hat_wls)
sigma2_wls <- sum(diag(solve(cov)) * res_wls^2) / (nrow(X_train) - ncol(X_train))
V_theta_wls <- sigma2_wls * solve(F_N)

# WLS Predictions
y_hat_wls_full <- X_full %*% theta_hat_wls

# WLS standard errors and intervals
se_pred_wls_full <- sqrt(sigma2_wls + diag(X_full %*% V_theta_wls %*% t(X_full)))
t_crit_wls <- qt(0.975, df = nrow(X_train) - ncol(X_train))

pred_int_wls_full <- cbind(
  lower = y_hat_wls_full - t_crit_wls * se_pred_wls_full,
  upper = y_hat_wls_full + t_crit_wls * se_pred_wls_full
)

df_wls_full <- data.frame(
  x = c(Dtrain$x, Dtest$x),
  yhat = y_hat_wls_full,
  lower = pred_int_wls_full[, 1],
  upper = pred_int_wls_full[, 2],
  Model = "WLS (Discounted, lambda=0.9)"
)

# Combine both models for easy plotting
df_models_full <- rbind(df_ols_full, df_wls_full)

# Combine actual data for plotting
df_actuals <- data.frame(
  x = c(Dtrain$x, Dtest$x),
  total = c(Dtrain$total, Dtest$total),
  Set = c(rep("Training Data", nrow(Dtrain)), rep("Test Data", nrow(Dtest)))
)

# ==========================================
# 3. Plotting the Full Models
# ==========================================
plot_4_5 <- ggplot() +
  # Actual observations
  geom_point(data = df_actuals, 
             aes(x = x, y = total, shape = Set), 
             color = "black", size = 1.5, alpha = 0.7) +
  
  # Lines for the fitted/forecasted models
  geom_line(data = df_models_full, 
            aes(x = x, y = yhat, color = Model), 
            linewidth = 1) +
  
  # Ribbons for the 95% prediction intervals
  geom_ribbon(data = df_models_full, 
              aes(x = x, ymin = lower, ymax = upper, fill = Model), 
              alpha = 0.15) +
  
  # Vertical line to show where training ends and test begins
  geom_vline(xintercept = min(Dtest$x), linetype = "dashed", color = "darkgray", linewidth = 0.8) +
  annotate("text", x = min(Dtest$x) + 0.05, y = mean(df_actuals$total), 
           label = "Forecast Start", angle = 90, vjust = -0.5, color = "darkgray") +
  
  # Formatting and aesthetics
  scale_shape_manual(name = "Data", values = c("Training Data" = 16, "Test Data" = 17)) +
  scale_color_manual(name = "Model", values = c("OLS (Global Trend)" = "blue", "WLS (Discounted, lambda=0.9)" = "red")) +
  scale_fill_manual(name = "Model", values = c("OLS (Global Trend)" = "blue", "WLS (Discounted, lambda=0.9)" = "red")) +
  
  labs(title = "Full Model Comparison: OLS vs. WLS (Training & Forecast)",
       x = "Year",
       y = "Total Registered Vehicles (Millions)") +
  
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.box = "vertical"
  )

plot_4_5
#ggsave('4_5.png', plot =plot_4_5, height = 4, width = 8, dpi=400 )
```
# 4.6
```{r}
# ==========================================
# 4.6: WLS for various lambdas
# ==========================================
lambdas_to_test <- c(0.99, 0.9, 0.8, 0.7, 0.6)


results_list <- list()
slopes_list <- list() # To inspect the slopes later


for (l in lambdas_to_test) {
  # 1. Calculate weights and Weight matrix W (inverse of covariance)
  w <- l^((nrow(Dtrain)-1):0)
  W <- diag(w)
  
  # 2. Estimate parameters (theta_hat)
  F_N <- t(X_train) %*% W %*% X_train
  h_N <- t(X_train) %*% W %*% Y_train
  theta_hat <- solve(F_N) %*% h_N
  
  # Save the slope (theta_2) for our commentary
  slopes_list[[as.character(l)]] <- theta_hat[2, 1]
  
  # 3. Forecasts for all data points
  y_hat_full <- X_full %*% theta_hat
  
  # 4. Store in a temporary dataframe
  tmp_df <- data.frame(
    x = c(Dtrain$x, Dtest$x),
    yhat = as.numeric(y_hat_full),
    Lambda = as.factor(paste("lambda =", l))
  )
  results_list[[as.character(l)]] <- tmp_df
}

# Combine all results into one dataframe
df_lambdas <- do.call(rbind, results_list)

# Print the slopes to answer the question
cat("Estimated Slopes (theta_2) for different lambdas:\n")
print(unlist(slopes_list))

# ==========================================
# Plotting the different lambdas
# ==========================================
plot_4_6 <- ggplot() +
  # Actual observations
  geom_point(data = df_actuals, 
             aes(x = x, y = total, shape = Set), 
             color = "black", size = 1.5, alpha = 0.5) +
  
  # Fitted/Forecasted lines for each lambda
  geom_line(data = df_lambdas, 
            aes(x = x, y = yhat, color = Lambda), 
            linewidth = 1) +
  
  # Vertical line separating train/test
  geom_vline(xintercept = min(Dtest$x), linetype = "dashed", color = "darkgray", linewidth = 0.8) +
  annotate("text", x = min(Dtest$x) + 0.05, y = mean(df_actuals$total), 
           label = "Forecast Start", angle = 90, vjust = -0.5, color = "darkgray") +
  
  scale_shape_manual(name = "Data", values = c("Training Data" = 16, "Test Data" = 17)) +
  labs(title = "WLS Forecasts for Different Forgetting Factors (Lambda)",
       x = "Year",
       y = "Total Registered Vehicles (Millions)") +
  
  theme_minimal() +
  theme(legend.position = "right")

plot_4_6
#ggsave('4_6.png', plot = plot_4_6, height = 4, width = 8, dpi=400)
```


# 5.1

```{r}

y

X
#cat("y1 =", y[1], "\n")  # Jan 2018
# cat("y2 =", y[2], "\n")  # Feb 2018
# cat("x1 =", X[1,], "\n")      # ~2018.0  
# cat("x2 =", X[2,], "\n")      # ~2018.083

```

# 5.2

```{r}
# RLS
# Set the initial value of R and theta
p <- ncol(X) 
N <- nrow(X) # datapoints
R <- diag(0.1, nrow=p)
```

```{r}
theta <- c(0,0)#rep(0, p)
Theta <- matrix(NA, nrow=N, ncol=p)

# Iterate through and estimate the parameters
for(i in 1:N){
  (x <- X[i, ])
  # Update
  (R <- R + x %*% t(x))
  (theta <- theta + solve(R) %*% x %*% (y[i] - t(x) %*% theta))
  Theta[i, ] <- theta
}

Theta

Theta[1,] 
Theta[2,]
Theta[3,]

```

# 5.3. Calculate the RLS estimates at time t= N (i.e.
θN) and compare them to the OLS estimates,
are they close? Can you find a way to decrease the difference by modifying some of the RLS
initial values and explain why initial values are important to get right?

# From what I understand we should retrieve the last value of the Thetas
```{r}
theta <- c(0,0)#rep(0, p)
Theta <- matrix(NA, nrow=N, ncol=p)

# Iterate through and estimate the parameters
for(i in 1:N){
  (x <- X[i, ])
  # Update
  (R <- R + x %*% t(x))
  (theta <- theta + solve(R) %*% x %*% (y[i] - t(x) %*% theta))
  Theta[i, ] <- theta
}

Theta
```




# 5.4 - Implement RLS with forgetting


```{r}
# Now iterate through data:
# For each step:
# - calculate R and theta
# - calculate one-step prediction

lambda <- 0.7

# we will use the entire dataset (not only the training data from before):
X <- cbind(1, Dtrain$x)
y <- cbind(Dtrain$total)

n <- length(X[,1])

# initialise containers for parameter estimates (Theta) and one step predictions:
Theta_07 <- matrix(NA, nrow=n, ncol=2)
OneStepPred_07 <- matrix(NA, nrow=n)


# 1 # very first step:
x1 <- X[1,]

R_1 <- diag(0.1,2) # Same initialization as before 
h_1 <- x1*y[1]    # h is a px1 vector (but R prints it in a row..)

# to estimate theta we need to invert R:
#solve(R_1)
# in this very first step R cannot be inverted - too soon to estimate parameters!
# (we cannot estimate p parameters drom only one datapoint)


# 2 # second step - first time to estimate parameters and make prediction
x2 <- X[2,]
R_2 <- lambda*R_1 + x2 %*% t(x2)
h_2 <- lambda*h_1 + x2 * y[2]

# we estimate theta (for the first time - so not yet using "update" formula):
Theta_07[2,] <- solve(R_2) %*% h_2

# we predict one step ahead:
OneStepPred_07[2+1] <- X[2+1,]%*%Theta_07[2,]


# 3 # third step - first time to use update formula
x3 <- X[3,]
R_3 <- lambda*R_2 + x3 %*% t(x3)
Theta_07[3,] <- Theta_07[2,] + solve(R_3) %*% x3 %*% (y[3] - t(x3) %*% Theta_07[2,])

# we predict one step ahead:
OneStepPred_07[3+1] <- X[3+1,]%*%Theta_07[3,]


# next many steps # - update and predict

R <- R_3

for(i in 4:n){
  x <- X[i, ]
  # Update
  R <- lambda*R + x %*% t(x)
  Theta_07[i, ] <- Theta_07[i-1, ] + solve(R) %*% x %*% (y[i] - t(x) %*% Theta_07[i-1, ])
}

# predict
for(i in 4:n-1){
  OneStepPred_07[i+1] <- X[i+1, ]%*%Theta_07[i, ]
}
```

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


```{r}
# Again for lambda = 0.99

lambda <- 0.99

# we will use the entire dataset (not only the training data from before):
X <- cbind(1, Dtrain$x)
y <- cbind(Dtrain$total)

n <- length(X[,1])

# initialise containers for parameter estimates (Theta) and one step predictions:
Theta_099 <- matrix(NA, nrow=n, ncol=2)
OneStepPred_099 <- matrix(NA, nrow=n)

x1 <- X[1,]

R_1 <- diag(0.1,2) # Initialize 
h_1 <- x1*y[1]    # h is a px1 vector (but R prints it in a row..)

# to estimate theta we need to invert R:
#solve(R_1)
# in this very first step R cannot be inverted - too soon to estimate parameters!
# (we cannot estimate p parameters drom only one datapoint)

# 2 # second step - first time to estimate parameters and make prediction
x2 <- X[2,]
R_2 <- lambda*R_1 + x2 %*% t(x2)
h_2 <- lambda*h_1 + x2 * y[2]

# we estimate theta (for the first time - so not yet using "update" formula):
Theta_099[2,] <- solve(R_2) %*% h_2

# we predict one step ahead:
OneStepPred_099[2+1] <- X[2+1,]%*%Theta_099[2,]


# 3 # third step - first time to use update formula
x3 <- X[3,]
R_3 <- lambda*R_2 + x3 %*% t(x3)
Theta_099[3,] <- Theta_099[2,] + solve(R_3) %*% x3 %*% (y[3] - t(x3) %*% Theta_099[2,])

# we predict one step ahead:
OneStepPred_099[3+1] <- X[3+1,]%*%Theta_099[3,]


# next many steps # - update and predict

R <- R_3

for(i in 4:n){
  x <- X[i, ]
  # Update
  R <- lambda*R + x %*% t(x)
  Theta_099[i, ] <- Theta_099[i-1, ] + solve(R) %*% x %*% (y[i] - t(x) %*% Theta_099[i-1, ])
}

# predict
for(i in 4:n-1){
  OneStepPred_099[i+1] <- X[i+1, ]%*%Theta_099[i, ]
}
```




```{r}
plot(Theta_07[,1], xlab = "Index", ylab = "Theta_1 value")
points(Theta_099[,1], col=2)
legend("topleft", legend=c("lambda=0.7","lambda=0.99"),
       col=c(1,2), lty=1)
```


```{r}
plot(Theta_07[,2], xlab = "Index", ylab = "Theta_2 value")
points(Theta_099[,2], col=2)
legend("topleft", legend=c("lambda=0.7","lambda=0.99"),
       col=c(1,2), lty=1)
```





# 5.5 - Residuals


```{r}
Theta_07[1,] =c(0,0)
Theta_099[1,] =c(0,0)
```

```{r}
# Plot one step predictions:
ggplot(Dtrain, aes(x=x, y=total)) +
  geom_point(col="black") +
  geom_point(aes(y=OneStepPred_07), colour="blue", size=1) + 
  geom_line(aes(y=OneStepPred_07), colour="blue") +
  coord_cartesian(ylim = c(2.9, 3.4), xlim = c(2018,2024)) +
  scale_colour_manual(
    name = "Series",
    values = c("Observed" = "black",
               "One-step prediction" = "blue")
  )

```



```{r}
residplot_07 <- OneStepPred_07[-(1:5)]
residplot_099 <- OneStepPred_099[-(1:5)]
residplot_y <- y[-(1:5)]
```

```{r}
one_step_resid_07 <- residplot_07 - residplot_y
one_step_resid_099 <- residplot_099 -residplot_y
```



```{r}
plot(one_step_resid_07, xlab = "Index", ylab = "Residuals", ylim = c(-0.12,0.2))
points(one_step_resid_099, col="red")
legend("topright", legend = c("lambda=0.7", "lambda=0.99"), col = c(1,2), lty=1)
```







# 5.6 - k-step RMSE



```{r}
# Just the RLS code from earlier but I made it a function
RLS_forgetting <- function(y,X,lambda){
n <- nrow(X)
p <- ncol(X)

# initialise containers for parameter estimates (Theta) and one step predictions:
Theta <- matrix(NA, nrow=n, ncol=p)
OneStepPred <- matrix(NA, nrow=n)


# 1 # very first step:
x1 <- X[1,]

R_1 <- diag(0.1,2) # Initialization
h_1 <- x1*y[1]    # h is a px1 vector (but R prints it in a row..)




# 2 # second step - first time to estimate parameters and make prediction
x2 <- X[2,]
R_2 <- lambda*R_1 + x2 %*% t(x2)
h_2 <- lambda*h_1 + x2 * y[2]


# R is now invertible (we can estimate p parameters from p observations)

# we estimate theta (for the first time - so not yet using "update" formula):
Theta[2,] <- solve(R_2) %*% h_2

# we predict one step ahead:
OneStepPred[2+1] <- X[2+1,]%*%Theta[2,]

# 3 # third step - first time to use update formula
x3 <- X[3,]
R_3 <- lambda*R_2 + x3 %*% t(x3)
Theta[3,] <- Theta[2,] + solve(R_3) %*% x3 %*% (y[3] - t(x3) %*% Theta[2,])

# we predict one step ahead:
OneStepPred[3+1] <- X[3+1,]%*%Theta[3,]

# next many steps # - update and predict

R <- R_3

for(i in 4:n){
  x <- X[i, ]
  # Update
  R <- lambda*R + x %*% t(x)
  Theta[i, ] <- Theta[i-1, ] + solve(R) %*% x %*% (y[i] - t(x) %*% Theta[i-1, ])
}

# predict
for(i in 4:n-1){
  OneStepPred[i+1] <- X[i+1, ]%*%Theta[i, ]
}

return(list(OneStepPred = OneStepPred,
            Theta = Theta))

}
```



```{r}
# Setup parameters
k_seq <- 1:12                              
lambda_seq <- seq(0.5, 0.99, by = 0.01)   

# Create a matrix to store results (Rows = lambda, Columns = k)
rmse_matrix <- matrix(NA, nrow = length(lambda_seq), ncol = length(k_seq))
n <- nrow(X)

# Loop through each lambda value
for (j in seq_along(lambda_seq)) {
  lam <- lambda_seq[j]
  
  # Fit the RLS model ONCE per lambda
  rls_model <- RLS_forgetting(y, X, lam)
  Theta_est <- rls_model$Theta
  
  # Loop through each k value for the current lambda
  for (k_idx in seq_along(k_seq)) {
    k <- k_seq[k_idx]
    
    k_step_preds <- rep(NA, n)
    
    # Calculate k-step predictions
    for (i in (k + 1):n) {
      if (!any(is.na(Theta_est[i - k, ]))) {
        k_step_preds[i] <- sum(X[i, ] * Theta_est[i - k, ]) 
      }
    }
    
    # Calculate residuals and store the RMSE in the matrix
    k_step_residuals <- y - k_step_preds
    rmse_matrix[j, k_idx] <- sqrt(mean(k_step_residuals^2, na.rm = TRUE))
  }
}

# Plot the results for multiple columns
colors <- rainbow(length(k_seq))

matplot(lambda_seq, rmse_matrix, 
        type = "l", lty = 1, lwd = 2, col = colors,
        xlab = expression(lambda), 
        ylab = "RMSE",
        main = "RMSE vs. Lambda across Forecast Horizons (k=1 to 12)")

legend("topright", 
       legend = paste("k =", k_seq), 
       col = colors, 
       lty = 1, 
       lwd = 2, 
       cex = 0.8,
       title = "Forecast Horizon")
```




# 5.7 - RLS Predictions



```{r}
y_test <- Dtest$total
```


```{r}
# I will choose lambda as 0.7
lambda_fixed <- 0.7

# Run on training set
train_results <- RLS_forgetting(y, X, lambda_fixed)

# Extract the very last Theta from training to use as the starting point for test
last_theta_train <- train_results$Theta[nrow(X), ]

# We also need the final R matrix from training. 
# Since the function doesn't return it, we quickly re-calculate it:
p <- ncol(X)
R <- diag(0.1, p) 
for(i in 1:nrow(X)) {
  x <- X[i, ]
  R <- lambda_fixed * R + x %*% t(x)
}
```




```{r}
n_test <- nrow(X_test)
test_preds <- numeric(n_test)
current_theta <- last_theta_train

for (i in 1:n_test) {
  x_test <- X_test[i, ]
  
  # 1. Predict the current test value using the existing theta
  test_preds[i] <- sum(x_test * current_theta)
  
  # 2. Update R and Theta using the actual y_test[i] (Online Learning)
  R <- lambda_fixed * R + x_test %*% t(x_test)
  current_theta <- current_theta + solve(R) %*% x_test %*% (y_test[i] - sum(x_test * current_theta))
}
```





```{r}
# Create the test data frame
# (Ensure x_test_values are the time points corresponding to your test set)
Dtest_plot <- data.frame(
  x = Dtest$x, 
  total = y_test,
  OneStepPred_07 = test_preds
)

Dtrain_plot <- data.frame(
  x = Dtrain$x, 
  total = y,
  OneStepPred_07 = OneStepPred_07
)

# Combine them into one "Full" dataset
Dfull <- rbind(Dtrain_plot[, c("x", "total", "OneStepPred_07")], Dtest_plot)

# Identify where the split happens for the vertical line
split_time <- max(Dtrain_plot$x)
```


```{r}

ggplot(Dfull, aes(x = x)) +
  # Actual Observed Data
  geom_point(aes(y = total, colour = "Observed"), alpha = 0.5) +
  
  # RLS Predictions (Train + Test)
  geom_line(aes(y = OneStepPred_07, colour = "One-step prediction"), size = 1) +
  geom_point(aes(y = OneStepPred_07, colour = "One-step prediction"), size = 1) +
  
  # Vertical line to show where Test set begins
  geom_vline(xintercept = split_time, linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = split_time, y = 3.35, label = "Test Start", color = "red", angle = 90, vjust = -0.5) +
  
  # Styling and Limits
  coord_cartesian(ylim = c(2.9, 3.4), xlim = c(2019, max(Dfull$x))) +
  scale_colour_manual(
    name = " ",
    values = c("Observed" = "black", "One-step prediction" = "blue")
  ) +
  theme_minimal() +
  labs(title = "RLS Predictions",
       subtitle = "Lambda = 0.7",
       x = "Year", y = "Total Registered Vehicles")
```

